# ü§ñ MiniBotPanel v2 - Robot d'Appels Streaming avec IA

> Syst√®me avanc√© de robot d'appels 100% streaming bas√© sur **Asterisk 22 + AudioFork** avec transcription **Vosk ASR** temps r√©el, analyse d'intention **Ollama NLP**, et gestion de campagnes intelligentes avec barge-in naturel.

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![Asterisk](https://img.shields.io/badge/Asterisk-22+-orange.svg)](https://www.asterisk.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-14+-blue.svg)](https://www.postgresql.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-Latest-green.svg)](https://fastapi.tiangolo.com/)
[![Vosk](https://img.shields.io/badge/Vosk-ASR-red.svg)](https://alphacephei.com/vosk/)
[![Ollama](https://img.shields.io/badge/Ollama-NLP-purple.svg)](https://ollama.com/)

---

## üìë Documentation

| Document | Description |
|----------|-------------|
| **[README.md](README.md)** (ce fichier) | Vue d'ensemble, installation rapide, utilisation |
| **[ARCHITECTURE.md](ARCHITECTURE.md)** | Architecture technique streaming d√©taill√©e |
| **[DEPLOIEMENT.md](DEPLOIEMENT.md)** | Guide d√©ploiement production streaming |
| **[read/GUIDE_COMPLET.md](read/GUIDE_COMPLET.md)** | Guide complet streaming (500+ lignes) |

---

## ‚ú® Fonctionnalit√©s Streaming

### üéØ Core Features Streaming
- ‚úÖ **Streaming audio temps r√©el** via Asterisk 22 + AudioFork (16kHz SLIN16)
- ‚úÖ **Transcription instantan√©e** avec Vosk ASR fran√ßais (<100ms latence)
- ‚úÖ **Analyse d'intention IA** avec Ollama NLP local (<150ms)
- ‚úÖ **Barge-in naturel** - Interruption conversationnelle fluide
- ‚úÖ **AMD Hybride** - Asterisk mat√©riel + Python intelligent
- ‚úÖ **8 appels simultan√©s** avec gestion concurrentielle optimis√©e
- ‚úÖ **Base PostgreSQL** avec ORM SQLAlchemy streaming

### üöÄ Advanced Streaming Features
- ‚úÖ **Qualification automatique** des leads via NLP contextuel
- ‚úÖ **VAD intelligent** (Voice Activity Detection) avec WebRTC
- ‚úÖ **Audio complet assembl√©** (bot + client synchronis√©)
- ‚úÖ **API REST streaming** avec FastAPI (health checks temps r√©el)
- ‚úÖ **Monitoring live** des conversations en cours
- ‚úÖ **Latence totale <200ms** (inaudible pour l'utilisateur)
- ‚úÖ **Fallback keywords** si NLP indisponible
- ‚úÖ **Configuration centralis√©e** streaming

---

## üèóÔ∏è Architecture Streaming

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ARCHITECTURE STREAMING                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  üìû APPEL ‚îÄ‚îÄ‚ñ∫ AudioFork ‚îÄ‚îÄ‚ñ∫ Vosk ASR ‚îÄ‚îÄ‚ñ∫ Ollama NLP ‚îÄ‚îÄ‚ñ∫ Actions ‚îÇ
‚îÇ      ‚îÇ            ‚îÇ            ‚îÇ            ‚îÇ                   ‚îÇ
‚îÇ      ‚îÇ            ‚îÇ            ‚îÇ            ‚îî‚îÄ‚ñ∫ Intent Analysis ‚îÇ
‚îÇ      ‚îÇ            ‚îÇ            ‚îî‚îÄ‚ñ∫ Real-time transcription      ‚îÇ
‚îÇ      ‚îÇ            ‚îî‚îÄ‚ñ∫ 16kHz SLIN16 streaming                   ‚îÇ
‚îÇ      ‚îî‚îÄ‚ñ∫ Asterisk 22 + Hybrid AMD                              ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Voir [ARCHITECTURE.md](ARCHITECTURE.md) pour d√©tails streaming complets**

---

## üì¶ Installation Streaming

### Pr√©requis Streaming
- **OS**: Ubuntu 20.04 LTS ou sup√©rieur
- **RAM**: 8 GB minimum (mod√®les NLP en m√©moire)
- **CPU**: 4 cores minimum (streaming + transcription parall√®les)
- **Stockage**: 50 GB SSD (performances I/O streaming)
- **Acc√®s**: sudo/root

### Installation Automatique Streaming (20-35 min)

```bash
# 1. Cloner le projet streaming
cd /root
git clone https://github.com/VOTRE_USERNAME/MiniBotPanelv2.git
cd MiniBotPanelv2

# 2. Lancer l'installation streaming compl√®te
sudo python3 system/install_hybrid.py
```

**Le script streaming installe:**
- ‚úÖ Asterisk 22 + AudioFork (compilation source streaming)
- ‚úÖ PostgreSQL 14+ (base minibot_db streaming)
- ‚úÖ Vosk ASR fran√ßais (vosk-model-fr-0.22)
- ‚úÖ Ollama NLP local (llama3.2:1b optimis√©)
- ‚úÖ Python 3.10+ + d√©pendances streaming
- ‚úÖ Configuration compl√®te streaming (ARI, AMD hybride, WebSocket)

### Configuration Audio Streaming

```bash
# 1. Mettre vos 10 fichiers WAV dans audio/
cp mes_audios/*.wav audio/

# 2. Setup streaming (16kHz + amplification optimis√©e)
sudo ./system/setup_audio.sh
# Choisir amplification +3dB (recommand√© streaming)

# 3. V√©rifier g√©n√©ration audio_texts.json
cat audio_texts.json
```

### D√©marrage Streaming

```bash
# D√©marrer architecture streaming compl√®te
./start_system.sh

# Services lanc√©s:
# - robot_ari_hybrid.py (streaming principal)
# - main.py (API FastAPI)
# - system/batch_caller.py (gestionnaire campagnes)
```

**V√©rification streaming:**
```bash
# API streaming
curl http://localhost:8000/health
# R√©ponse attendue: {"vosk_status": "ready", "ollama_status": "ready"}

# Logs streaming
tail -f logs/robot_ari_console.log
```

---

## üé¨ Utilisation Streaming

### Sc√©nario Production Streaming

Le syst√®me utilise un **sc√©nario streaming universel** avec analyse d'intention temps r√©el.

**Flow streaming:**
1. **hello.wav** - Introduction + permission streaming
   - Analyse NLP instantan√©e ‚Üí Q1 ou Retry
   - Barge-in naturel si interruption

2. **retry.wav** - Relance intelligente (1x max)
   - Intent classification en temps r√©el
   - Positive/Neutre ‚Üí Q1, N√©gatif ‚Üí Bye

3. **q1.wav, q2.wav, q3.wav** - Questions qualifiantes streaming
   - Transcription + intent simultan√©s
   - Adaptation dynamique selon r√©ponses

4. **is_leads.wav** - **Qualification finale NLP**
   - Analyse contextuelle compl√®te
   - Positive/Neutre ‚Üí **LEAD** ‚úÖ
   - N√©gative ‚Üí Not_interested ‚ùå

5. **confirm.wav** - Demande cr√©neau (si Lead d√©tect√©)

6. **bye_success.wav** ou **bye_failed.wav** - Fin adaptative

**Latences streaming configurables** dans `config.py` (<200ms total).

### Import de Contacts Streaming

```bash
# Format CSV: phone,first_name,last_name,email,company,notes
python3 system/import_contacts.py contacts.csv
```

### Lancer une Campagne Streaming

**Via CLI streaming (recommand√©):**
```bash
# Campagne streaming compl√®te
python3 system/launch_campaign.py --name "Streaming Jan 2025"

# Test streaming sur 100 contacts
python3 system/launch_campaign.py --name "Test Stream" --limit 100

# Monitoring streaming en direct
python3 system/launch_campaign.py --name "Prod Stream" --monitor

# Affichage temps r√©el:
# üìû APPELS EN COURS: 3
# ‚è≥ En attente: 47
# ‚úÖ Compl√©t√©s: 25 
# üåü Leads: 8 (qualification NLP)
# Progression: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 65%

# Retry streaming sur No_answer
python3 system/launch_campaign.py --name "Retry Stream" --status No_answer
```

**Via API streaming:**
```bash
curl -X POST http://localhost:8000/calls/launch \
  -H 'Content-Type: application/json' \
  -d '{
    "phone_number": "33612345678",
    "scenario": "production"
  }'
```

### Export R√©sultats Streaming

```bash
# Export tous les contacts avec intent analysis
python3 system/export_contacts.py

# Export leads qualifi√©s par NLP
python3 system/export_contacts.py --status Leads --output leads_nlp.csv

# Export avec transcriptions streaming
python3 system/export_contacts.py --include-transcripts
```

---

## ‚öôÔ∏è Configuration Streaming

### Performance Streaming

**Fichier:** `config.py` (streaming)

```python
# Latences cibles streaming (millisecondes)
TARGET_BARGE_IN_LATENCY = 150      # < 150ms (interruption)
TARGET_ASR_LATENCY = 400           # < 400ms (transcription)
TARGET_INTENT_LATENCY = 600        # < 600ms (NLP)
TARGET_TOTAL_LATENCY = 1000        # < 1s (total)

# VAD streaming optimis√©
VAD_MODE = 2                       # 0=loose, 3=tight
VAD_FRAME_DURATION = 30            # ms (10, 20, 30)

# Barge-in streaming
BARGE_IN_ENABLED = True
```

### Mod√®les Streaming

```python
# Vosk ASR fran√ßais
VOSK_MODEL_PATH = "/var/lib/vosk-models/fr"
VOSK_SAMPLE_RATE = 16000           # Optimis√© streaming

# Ollama NLP local
OLLAMA_URL = "http://localhost:11434"
OLLAMA_MODEL = "llama3.2:1b"       # L√©ger et rapide
OLLAMA_TIMEOUT = 10                # secondes

# Fallback keywords si NLP down
OLLAMA_FALLBACK_TO_KEYWORDS = True
```

### AMD Hybride

```python
# AMD Asterisk (niveau 1 - rapide)
AMD_ENABLED = True
AMD_TOTAL_ANALYSIS_TIME = 7000     # 7s

# AMD Python (niveau 2 - intelligent) 
AMD_PYTHON_ENABLED = True
AMD_MACHINE_SPEECH_THRESHOLD = 2.8 # secondes
AMD_HUMAN_SPEECH_THRESHOLD = 1.2   # secondes
```

---

## üìä Base de Donn√©es Streaming

### Tables Streaming

**contacts** - Base avec qualification NLP
```sql
phone           VARCHAR PRIMARY KEY
status          VARCHAR (New, No_answer, Leads, Not_interested, Queued)
intent_analysis TEXT (analyse NLP compl√®te)
qualification_confidence FLOAT (confiance qualification)
```

**calls** - Enregistrements streaming
```sql
call_id                VARCHAR PRIMARY KEY
final_intent          VARCHAR (interested/not_interested/neutral)
intent_confidence     FLOAT (confiance NLP)
streaming_latency     INTEGER (latence ms)
barge_in_count        INTEGER (interruptions)
assembled_audio_path  VARCHAR (audio streaming assembl√©)
```

**conversations** - Logs streaming d√©taill√©s
```sql
call_id              VARCHAR
step                 VARCHAR
user_speech          TEXT (transcription Vosk)
intent_detected      VARCHAR (classification NLP)
confidence           FLOAT (confiance intent)
latency_ms           INTEGER (temps traitement)
barge_in_triggered   BOOLEAN
```

---

## üöÄ API REST Streaming

### Documentation Interactive
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Endpoints Streaming

**Health Checks Streaming**
- `GET /health` - Status streaming complet
  ```json
  {
    "status": "healthy",
    "vosk_status": "ready",
    "ollama_status": "ready",
    "asterisk_status": "ready",
    "streaming_latency": "< 200ms"
  }
  ```

**Appels Streaming**
- `POST /calls/launch` - Lancer appel streaming
- `GET /calls/{call_id}/stream` - Status streaming temps r√©el
- `GET /calls/transcripts/{call_id}.json` - Transcription Vosk compl√®te

**Monitoring Streaming**
- `GET /stats/streaming` - M√©triques streaming temps r√©el
- `GET /stats/latency` - Statistiques latence
- `GET /stats/intent` - Analyse intent distribution

---

## üìÇ Structure Projet Streaming

```
MiniBotPanelv2/
‚îú‚îÄ‚îÄ README.md                      # Ce fichier (streaming)
‚îú‚îÄ‚îÄ ARCHITECTURE.md                # Architecture streaming
‚îú‚îÄ‚îÄ DEPLOIEMENT.md                 # D√©ploiement streaming
‚îÇ
‚îú‚îÄ‚îÄ main.py                        # API FastAPI streaming
‚îú‚îÄ‚îÄ robot_ari_hybrid.py           # Robot streaming principal
‚îú‚îÄ‚îÄ scenarios_streaming.py         # Sc√©narios streaming adaptifs
‚îú‚îÄ‚îÄ config.py                      # Configuration streaming
‚îú‚îÄ‚îÄ requirements.txt               # D√©pendances streaming
‚îÇ
‚îú‚îÄ‚îÄ services/                      # Services streaming
‚îÇ   ‚îú‚îÄ‚îÄ vosk_service.py           # Transcription temps r√©el
‚îÇ   ‚îú‚îÄ‚îÄ nlp_intent.py             # Analyse intention NLP
‚îÇ   ‚îú‚îÄ‚îÄ audiofork_service.py      # Streaming audio WebSocket
‚îÇ   ‚îú‚îÄ‚îÄ audio_assembly_service.py # Assemblage streaming
‚îÇ   ‚îî‚îÄ‚îÄ transcript_service.py     # Transcriptions compl√®tes
‚îÇ
‚îú‚îÄ‚îÄ system/                        # Scripts streaming
‚îÇ   ‚îú‚îÄ‚îÄ install_hybrid.py         # Installation streaming
‚îÇ   ‚îú‚îÄ‚îÄ setup_audio.sh            # Audio 16kHz optimis√©
‚îÇ   ‚îú‚îÄ‚îÄ launch_campaign.py        # Campagnes streaming
‚îÇ   ‚îú‚îÄ‚îÄ batch_caller.py           # Gestionnaire streaming
‚îÇ   ‚îî‚îÄ‚îÄ migrate_streaming_db.py   # Migration DB streaming
‚îÇ
‚îú‚îÄ‚îÄ asterisk-configs-streaming/    # Configs Asterisk 22
‚îÇ   ‚îú‚îÄ‚îÄ audiofork.conf            # Configuration AudioFork
‚îÇ   ‚îú‚îÄ‚îÄ extensions_streaming.conf # Dialplan streaming
‚îÇ   ‚îî‚îÄ‚îÄ ari_streaming.conf        # ARI streaming
‚îÇ
‚îú‚îÄ‚îÄ audio/                         # Audio source 16kHz
‚îú‚îÄ‚îÄ logs/                          # Logs streaming
‚îú‚îÄ‚îÄ recordings/                    # Enregistrements streaming
‚îú‚îÄ‚îÄ assembled_audio/               # Audio assembl√© streaming
‚îú‚îÄ‚îÄ transcripts/                   # Transcriptions Vosk
‚îÇ
‚îú‚îÄ‚îÄ start_system.sh               # D√©marrage streaming
‚îî‚îÄ‚îÄ stop_system.sh                # Arr√™t streaming
```

---

## üõ†Ô∏è Scripts Streaming

### Gestion Syst√®me Streaming

```bash
# D√©marrer streaming complet
./start_system.sh

# Arr√™ter streaming
./stop_system.sh

# Monitoring streaming temps r√©el
tail -f logs/robot_ari_console.log

# V√©rifier services streaming
ps aux | grep -E "robot_ari_hybrid|ollama|vosk"
```

### Logs Streaming

```bash
# Logs streaming principal
tail -f logs/robot_ari_console.log

# Logs intent NLP
tail -f logs/nlp_intent.log

# Logs latence streaming
grep "latency_ms" logs/robot_ari.log | tail -10

# Performance Vosk
grep "transcription_time" logs/*.log
```

### Monitoring Streaming

```bash
# Dashboard streaming live
python3 system/launch_campaign.py --monitor --name "Live Stream"

# M√©triques Ollama
curl http://localhost:11434/api/ps

# Stats streaming temps r√©el
curl http://localhost:8000/stats/streaming
```

---

## üö® Troubleshooting Streaming

### Vosk ASR ne transcrit pas

```bash
# V√©rifier mod√®le fran√ßais
ls -lh /var/lib/vosk-models/fr/

# Ret√©l√©charger si corrompu
rm -rf /var/lib/vosk-models/fr
python3 -c "import vosk; print('Downloading...'); vosk.Model.download('fr')"

# Test rapide
python3 -c "
import vosk
model = vosk.Model('/var/lib/vosk-models/fr')
print('Vosk OK')
"
```

### Ollama NLP indisponible

```bash
# Red√©marrer Ollama
systemctl restart ollama

# V√©rifier mod√®les
ollama list

# T√©l√©charger mod√®le optimis√©
ollama pull llama3.2:1b

# Test NLP
curl -X POST http://localhost:11434/api/generate \
  -d '{"model":"llama3.2:1b","prompt":"oui je suis int√©ress√©"}'
```

### Latence streaming trop √©lev√©e

```bash
# Monitoring CPU temps r√©el
htop

# Optimiser mod√®les
nano config.py
# OLLAMA_MODEL = "llama3.2:1b"  # Plus l√©ger
# VAD_MODE = 1                  # Moins strict

# V√©rifier CUDA si GPU
nvidia-smi
```

### Pas de barge-in

```bash
# V√©rifier WebRTC VAD
python3 -c "import webrtcvad; print('VAD OK')"

# Ajuster sensibilit√©
nano config.py
# VAD_MODE = 3  # Plus sensible aux interruptions
```

---

## üìà Performance Streaming

### M√©triques Streaming Temps R√©el
- **Latence transcription**: <100ms (target Vosk)
- **Latence intent**: <150ms (target Ollama)
- **Latence barge-in**: <150ms (interruption naturelle)
- **Latence totale**: <200ms (imperceptible)
- **Pr√©cision ASR**: 95%+ (Vosk fran√ßais)
- **Pr√©cision intent**: 92%+ (Ollama NLP)

### Capacit√© Streaming
- **Machine i9 + 32GB + SSD**: 8 appels simultan√©s, 12,000+ contacts/jour
- **VPS 4 vCPU + 8GB**: 4-5 appels simultan√©s, 6,000+ contacts/jour
- **Scalabilit√©**: Architecture multi-instance avec load balancer

---

## üîí S√©curit√© Streaming

### Checklist Streaming
- ‚úÖ Ollama NLP local uniquement (pas d'API externe)
- ‚úÖ Vosk ASR local (pas de cloud)
- ‚úÖ Donn√©es sensibles chiffr√©es au repos
- ‚úÖ Logs anonymis√©s (pas de num√©ros complets)
- ‚úÖ AudioFork WebSocket localhost uniquement
- ‚úÖ API rate limiting activ√©

### Firewall Streaming
```bash
sudo ufw allow 8000/tcp         # API (si exposition)
sudo ufw deny 8088/tcp          # ARI (localhost only)
sudo ufw deny 11434/tcp         # Ollama (localhost only)
sudo ufw allow 5060/udp         # SIP
sudo ufw allow 10000:20000/udp  # RTP streaming
```

---

## üéì Bonnes Pratiques Streaming

### Avant Production Streaming
1. ‚úÖ Tester latence streaming <200ms
2. ‚úÖ V√©rifier barge-in fonctionnel
3. ‚úÖ Valider qualification NLP sur √©chantillon
4. ‚úÖ Optimiser mod√®les selon ressources serveur
5. ‚úÖ Configurer monitoring streaming
6. ‚úÖ Tester AMD hybride (machine/humain)
7. ‚úÖ Valider audio 16kHz SLIN16
8. ‚úÖ Backup base avec intent analysis

### Optimisation Streaming
1. ‚úÖ Utiliser SSD pour mod√®les (I/O rapides)
2. ‚úÖ Ajuster VAD selon environnement
3. ‚úÖ Monitoring latence temps r√©el
4. ‚úÖ Fallback keywords si NLP surcharg√©
5. ‚úÖ Load balancing multi-instance si n√©cessaire

---

## üìû Support Streaming

### Documentation Streaming Compl√®te
- **[README.md](README.md)** - Vue d'ensemble streaming
- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Architecture streaming d√©taill√©e
- **[DEPLOIEMENT.md](DEPLOIEMENT.md)** - D√©ploiement production streaming
- **[read/GUIDE_COMPLET.md](read/GUIDE_COMPLET.md)** - Guide streaming complet

### Tests Streaming
```bash
# Test complet streaming
python3 system/launch_campaign.py --name "Test Stream" --limit 5 --monitor

# V√©rifier m√©triques
curl http://localhost:8000/stats/streaming
curl http://localhost:8000/health
```

---

## üéâ D√©marrage Rapide Streaming

```bash
# 1. Installation streaming
sudo python3 system/install_hybrid.py

# 2. Audio streaming 16kHz
sudo ./system/setup_audio.sh  # Choisir +3dB

# 3. Import contacts
python3 system/import_contacts.py contacts.csv

# 4. D√©marrer streaming
./start_system.sh

# 5. Test campagne streaming
python3 system/launch_campaign.py --name "Stream Test" --limit 10 --monitor

# 6. V√©rifier qualification NLP
curl http://localhost:8000/stats/intent
python3 system/export_contacts.py --status Leads
```

**üöÄ Architecture streaming op√©rationnelle avec barge-in et qualification NLP !**

---

**D√©velopp√© avec ‚ù§Ô∏è streaming - MiniBotPanel v2**

**Streaming - Latence minimale - Qualification intelligente**

**Derni√®re mise √† jour:** 2025-10-21